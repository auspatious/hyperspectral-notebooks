{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hvplot.xarray  # noqa: F401\n",
    "import numpy as np\n",
    "from fsspec.implementations.http import HTTPFileSystem\n",
    "from dea_tools.spatial import xr_vectorize\n",
    "import geohash\n",
    "import folium\n",
    "import odc.geo.xr\n",
    "from dea_tools.spatial import xr_rasterize\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import xarray as xr\n",
    "import json\n",
    "import shapely\n",
    "\n",
    "from emit_tools import emit_xarray\n",
    "from utils import get_rgb_dataset, get_earthdata_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See README.md for instructions on how to get an Earthdata token\n",
    "token = get_earthdata_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loading data can take around 3-4 minutes on a 100 Mbps connection\n",
    "\n",
    "# Refer to the README.md for instructions on how to find granule IDs\n",
    "granule = \"EMIT_L2A_RFL_001_20230131T221923_2303114_008\"  # Lake Charm\n",
    "\n",
    "s3_url = \"s3://lp-prod-protected/EMITL2ARFL.001/\" + granule + \"/\" + granule + \".nc\"\n",
    "http_url = s3_url.replace(\"s3://\", \"https://data.lpdaac.earthdatacloud.nasa.gov/\")\n",
    "\n",
    "fs = HTTPFileSystem(headers={\n",
    "    \"Authorization\": f\"bearer {token}\"\n",
    "})\n",
    "ds = emit_xarray(fs.open(http_url))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up empty bands.\n",
    "ds = ds.fillna(np.nan).where(ds.reflectance!=-0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a water layer\n",
    "high = ds.reflectance.sel(bands=450, method=\"nearest\")\n",
    "low = ds.reflectance.sel(bands=1275, method=\"nearest\")\n",
    "\n",
    "water = ((high - low) / (high + low)) > 0\n",
    "ds[\"water\"] = water.fillna(float(\"nan\")).where(water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.water.hvplot(aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_AREA = 80  # Hectares\n",
    "\n",
    "def add_geohash(row):\n",
    "    return geohash.encode(row.geometry.centroid.y, row.geometry.centroid.x, precision=9)\n",
    "\n",
    "    \n",
    "# Create polygons from the water layer\n",
    "water_polygons = xr_vectorize(ds.water, crs=\"epsg:4326\", mask=ds.water.values==1)\n",
    "water_polygons[\"area\"] = water_polygons.to_crs(\"epsg:3577\").area / 10000\n",
    "\n",
    "# Drop geopandas rows where the area is less than MIN_AREA\n",
    "water_polygons = water_polygons.drop(water_polygons[water_polygons['area'] < MIN_AREA].index)\n",
    "\n",
    "# Compute a geohash for each polygon at level 9\n",
    "geohashes = []\n",
    "for _, row in water_polygons.iterrows():\n",
    "    geohashes.append(add_geohash(row))\n",
    "\n",
    "water_polygons[\"geohash\"] = geohashes\n",
    "\n",
    "# Add an ID row\n",
    "water_polygons['id'] = range(1, water_polygons.shape[0] + 1)\n",
    "\n",
    "# Show us what we've got\n",
    "print(f\"Found {water_polygons.shape[0]} water polygons that are larger than {MIN_AREA} hectare(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View the water layer on an interactive map\n",
    "m = folium.Map(control_scale=True, tiles=None)\n",
    "\n",
    "for _, row in water_polygons.iterrows():\n",
    "    geojson = folium.GeoJson(\n",
    "        data=json.dumps(shapely.geometry.mapping(row.geometry)),\n",
    "        style_function=lambda x: {\"fillColor\": \"blue\", \"Color\": \"blue\"},\n",
    "        tooltip=f\"{row.geohash}\"\n",
    "    )\n",
    "    folium.Popup(f\"<p><strong>geohash:</strong> {row.geohash}<br><strong>area:</strong> {row['area']:.3f} Ha</p>\").add_to(\n",
    "        geojson\n",
    "    )\n",
    "    geojson.add_to(m)\n",
    "\n",
    "# Zoom map\n",
    "m.fit_bounds(ds.odc.map_bounds())\n",
    "\n",
    "tile = folium.TileLayer(\n",
    "    tiles=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
    "    attr=\"Esri\",\n",
    "    name=\"Esri Satellite\",\n",
    "    control=True,\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rasterise the polygons again, so we can join on the geohash later\n",
    "water_raster = xr_rasterize(water_polygons, ds, attribute_col=\"id\", crs=\"epsg:4326\")\n",
    "\n",
    "# Join the rasterised polygons to the dataset\n",
    "ds[\"id\"] = xr.DataArray(water_raster, dims=(\"latitude\", \"longitude\"))\n",
    "\n",
    "# Create another empty array of strings\n",
    "ds[\"geohash\"] = xr.DataArray(\n",
    "      np.full((ds.latitude.size, ds.longitude.size), \"\", dtype=\"U9\"),\n",
    "      dims=(\"latitude\", \"longitude\"),\n",
    ")\n",
    "\n",
    "for _, row in water_polygons.iterrows():\n",
    "   # I think 'where' works the opposite of what you'd expect\n",
    "   ds[\"geohash\"] = ds.geohash.where(ds.id != row.id, row.geohash)\n",
    "\n",
    "# Mask the empty values\n",
    "ds[\"geohash\"] = ds.geohash.where(ds.geohash != \"\", drop=False)\n",
    "del ds[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "means = ds.groupby(\"geohash\").mean()\n",
    "std_dev = ds.groupby(\"geohash\").std()\n",
    "min = ds.groupby(\"geohash\").min()\n",
    "max = ds.groupby(\"geohash\").max()\n",
    "\n",
    "# Create a new dataset with the mean, standard deviation, min and max values\n",
    "# for each geohash\n",
    "water_summaries = xr.Dataset(\n",
    "    {\n",
    "        \"mean\": means.reflectance,\n",
    "        \"std_dev\": std_dev.reflectance,\n",
    "        \"min\": min.reflectance,\n",
    "        \"max\": max.reflectance,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = water_summaries.sel(geohash=\"r1wjpxgbp\")\n",
    "hv.Spread(\n",
    "                row,\n",
    "                kdims=[\"bands\"],\n",
    "                vdims=[\"std_dev\", \"std_dev\"],\n",
    "                label=f\"{geohash}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and min-max plots\n",
    "color_cycle = hv.Cycle(\"Category20\")\n",
    "\n",
    "plots = []\n",
    "for geohash in water_summaries.geohash.values:\n",
    "    row = water_summaries.sel(geohash=geohash)\n",
    "\n",
    "    plots.append(\n",
    "        (\n",
    "            hv.Spread(\n",
    "                row,\n",
    "                vdims=[\"mean\", \"std_dev\", \"std_dev\"],\n",
    "                label=f\"{geohash}\"\n",
    "            )\n",
    "            * hv.Curve(\n",
    "                row,\n",
    "                vdims=\"mean\",\n",
    "                label=f\"{geohash}\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "hv.Layout(plots).opts(\n",
    "    opts.Spread(color=color_cycle, show_legend=True),\n",
    "    opts.Curve(color=color_cycle, show_legend=True),\n",
    "    opts.Overlay(\n",
    "        show_title=True, frame_width=200, frame_height=50, show_legend=False, yaxis=None\n",
    "    ),\n",
    ").cols(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absorption depth, to be plotted per geohash\n",
    "absorption = 627\n",
    "reference_band = 560\n",
    "reference_band2 = 648\n",
    "\n",
    "absorption = ds.reflectance.sel(bands=absorption, method = 'nearest')\n",
    "reference1= ds.reflectance.sel(bands=reference_band, method = 'nearest')\n",
    "reference2 = ds.reflectance.sel(bands=reference_band2, method = 'nearest')\n",
    "ds[\"absorption_depth\"] = (reference1 + reference2)/2 - absorption\n",
    "\n",
    "# Simplify to a summary dataset, removing the bands dimension and reflectance data\n",
    "ds_summary = ds.drop_dims(\"bands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Violin plots grouped by geohash\n",
    "ds_summary.hvplot.violin(\n",
    "    y=\"absorption_depth\",\n",
    "    by=\"geohash\",\n",
    ").opts(\n",
    "    opts.Violin(\n",
    "        width=800,\n",
    "        height=600,\n",
    "        xrotation=45,\n",
    "        show_legend=False,\n",
    "        title=\"Absorption Depth\",\n",
    "        ylim=(-0.02, 0.03),\n",
    "        violin_fill_color='absorption_depth',\n",
    "        cmap = 'Spectral_r',\n",
    "        clim = (0, 0.02),\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alex Test Environment",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
